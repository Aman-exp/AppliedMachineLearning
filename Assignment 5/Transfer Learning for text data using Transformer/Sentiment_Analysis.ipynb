{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRX4j5VTz24d"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2ce9842"
      },
      "source": [
        "# Dataset Download & Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge4DI5UOy42j",
        "outputId": "d9fcc6b5-aab6-4b5d-9608-4609a7255932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 54.4M  100 54.4M    0     0  54.5M      0 --:--:-- --:--:-- --:--:--  188M\n"
          ]
        }
      ],
      "source": [
        "!curl -L -o sentiment-analysis-dataset.zip https://www.kaggle.com/api/v1/datasets/download/abhi8923shriv/sentiment-analysis-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l09C59Fe0pSj"
      },
      "outputs": [],
      "source": [
        "!unzip -q sentiment-analysis-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbd18bc3"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fwjVgFQS0ryy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast, logging\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# optimizer from hugging face transformers\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f384115f"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XV-LYmRJ01ry"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"train.csv\", encoding='latin1')\n",
        "test_df = pd.read_csv(\"test.csv\", encoding='latin1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofe47kQ01CnY"
      },
      "source": [
        "## Unique Sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brrYOF1j09Od",
        "outputId": "a047fced-97c1-48d5-85b8-a19ff9d9c332"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['neutral', 'positive', 'negative'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "test_df = test_df.dropna(subset=['sentiment'])\n",
        "test_df['sentiment'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2b8e80b"
      },
      "source": [
        "## Tokenization & Input Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-n93yX7P1OB6"
      },
      "outputs": [],
      "source": [
        "# Create a mapping for sentiment labels\n",
        "sentiment_mapping = {\n",
        "    'positive': 0,\n",
        "    'negative': 1,\n",
        "    'neutral': 2\n",
        "}\n",
        "\n",
        "# Apply the mapping to create the 'label' column in train_df\n",
        "train_df['label'] = train_df['sentiment'].map(sentiment_mapping)\n",
        "\n",
        "# Apply the mapping to create the 'label' column in test_df\n",
        "test_df['label'] = test_df['sentiment'].map(sentiment_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "84aq5bu-1Puy"
      },
      "outputs": [],
      "source": [
        "train_text, val_text, train_labels, val_labels = train_test_split(train_df['text'], train_df['label'],\n",
        "                                                                    random_state=1000,\n",
        "                                                                    test_size=0.3,\n",
        "                                                                    stratify=train_df['label'])\n",
        "\n",
        "\n",
        "test_text, test_labels = test_df['text'], test_df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyU85Khh1V1A",
        "outputId": "3bf5fa50-cc6b-4cde-cd56-cf99eb755559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1045, 7078, 3866, 1996, 3185, 999, 1996, 3772, 2001, 10392, 1998, 1996, 2466, 2001, 7244, 1012, 102, 0], [101, 2054, 1037, 6659, 4031, 1012, 2009, 3631, 2044, 2074, 2048, 2420, 1012, 2561, 5949, 1997, 2769, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "# Suppress Hugging Face and Transformers logs, warnings, and download bars\n",
        "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sample data\n",
        "text = [\n",
        "    \"I absolutely loved the movie! The acting was fantastic and the story was touching.\",\n",
        "    \"What a terrible product. It broke after just two days. Total waste of money.\"\n",
        "]\n",
        "\n",
        "# Encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True)\n",
        "\n",
        "# Output\n",
        "print(sent_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "9tJfsktW1eRT",
        "outputId": "f4cbfb33-e8d6-4bbd-896c-71054a81d39a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "seq_len = [len((str)(i).split()) for i in train_text]\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51SD1rBP71BM",
        "outputId": "27296df5-6013-4a9c-ca63-ac870f90f575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "MAX_SEQ_LEN = 40\n",
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    [str(text) for text in train_text],\n",
        "    max_length = MAX_SEQ_LEN,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    [str(text) for text in val_text],\n",
        "    max_length = MAX_SEQ_LEN,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    [str(text) for text in test_text],\n",
        "    max_length = MAX_SEQ_LEN,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00084fdd"
      },
      "source": [
        "## Freezing BERT Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1eaq54L371mF"
      },
      "outputs": [],
      "source": [
        "## convert lists to tensors\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57976785"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oiMqoQHw78nJ"
      },
      "outputs": [],
      "source": [
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a91caa4b"
      },
      "source": [
        "## Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Sb4oawNV8Iea"
      },
      "outputs": [],
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "109c0e78"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DMEYPGhT8Kaj"
      },
      "outputs": [],
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "\n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert\n",
        "\n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "\n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,3)\n",
        "\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "      #pass the inputs to the model\n",
        "      out = self.bert(sent_id, attention_mask=mask)\n",
        "\n",
        "      x = self.fc1(out.pooler_output)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s8ct1sg08NDq"
      },
      "outputs": [],
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DiK_WG3M8VeH"
      },
      "outputs": [],
      "source": [
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 5e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07425c67"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EHPXqex8u4N",
        "outputId": "9a04115f-cb82-4934-ffef-50331be8c689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights: [1.06742134 1.17716174 0.82395271]\n"
          ]
        }
      ],
      "source": [
        "#compute the class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "print(\"Class Weights:\",class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c6a645b"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "c2vSNBjE865T"
      },
      "outputs": [],
      "source": [
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "weights = weights.to(device)\n",
        "\n",
        "cross_entropy  = nn.NLLLoss(weight=weights)\n",
        "\n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jq0_suX-87UJ"
      },
      "outputs": [],
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "\n",
        "  total_preds=[]\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74812395"
      },
      "source": [
        "## Loading Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3T5w5PCa8-tB"
      },
      "outputs": [],
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "\n",
        "  print(\"\\nEvaluating...\")\n",
        "\n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "\n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "\n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader)\n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b3ca9e1"
      },
      "source": [
        "## Predictions on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scsCMgNA9Bc6",
        "outputId": "e2d51030-6425-4213-f76e-9e9f61fe473e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 1.026\n",
            "Validation Loss: 0.939\n",
            "\n",
            " Epoch 2 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.938\n",
            "Validation Loss: 0.935\n",
            "\n",
            " Epoch 3 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.912\n",
            "Validation Loss: 0.883\n",
            "\n",
            " Epoch 4 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.900\n",
            "Validation Loss: 0.856\n",
            "\n",
            " Epoch 5 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.889\n",
            "Validation Loss: 0.841\n",
            "\n",
            " Epoch 6 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.874\n",
            "Validation Loss: 0.934\n",
            "\n",
            " Epoch 7 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.879\n",
            "Validation Loss: 0.828\n",
            "\n",
            " Epoch 8 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.874\n",
            "Validation Loss: 0.869\n",
            "\n",
            " Epoch 9 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.864\n",
            "Validation Loss: 0.831\n",
            "\n",
            " Epoch 10 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.860\n",
            "Validation Loss: 0.829\n",
            "\n",
            " Epoch 11 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.855\n",
            "Validation Loss: 0.819\n",
            "\n",
            " Epoch 12 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.856\n",
            "Validation Loss: 0.840\n",
            "\n",
            " Epoch 13 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.849\n",
            "Validation Loss: 0.817\n",
            "\n",
            " Epoch 14 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.854\n",
            "Validation Loss: 0.829\n",
            "\n",
            " Epoch 15 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.853\n",
            "Validation Loss: 0.824\n",
            "\n",
            " Epoch 16 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.851\n",
            "Validation Loss: 0.813\n",
            "\n",
            " Epoch 17 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.849\n",
            "Validation Loss: 0.821\n",
            "\n",
            " Epoch 18 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.847\n",
            "Validation Loss: 0.814\n",
            "\n",
            " Epoch 19 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.843\n",
            "Validation Loss: 0.830\n",
            "\n",
            " Epoch 20 / 20\n",
            "  Batch    50  of    602.\n",
            "  Batch   100  of    602.\n",
            "  Batch   150  of    602.\n",
            "  Batch   200  of    602.\n",
            "  Batch   250  of    602.\n",
            "  Batch   300  of    602.\n",
            "  Batch   350  of    602.\n",
            "  Batch   400  of    602.\n",
            "  Batch   450  of    602.\n",
            "  Batch   500  of    602.\n",
            "  Batch   550  of    602.\n",
            "  Batch   600  of    602.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    258.\n",
            "  Batch   100  of    258.\n",
            "  Batch   150  of    258.\n",
            "  Batch   200  of    258.\n",
            "  Batch   250  of    258.\n",
            "\n",
            "Training Loss: 0.844\n",
            "Validation Loss: 0.816\n"
          ]
        }
      ],
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "\n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "\n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VVA-B0H_Ciq",
        "outputId": "8ed205db-dacc-451c-912b-1bc552706ff5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1Ec3Gle6EuBk"
      },
      "outputs": [],
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "coeSDahQEzsx"
      },
      "outputs": [],
      "source": [
        "preds = np.argmax(preds, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibKQOSooE4_S",
        "outputId": "4f9b7412-d712-4e6c-be42-1f9e0383ccec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.66      0.67      1103\n",
            "           1       0.54      0.80      0.64      1001\n",
            "           2       0.65      0.45      0.53      1430\n",
            "\n",
            "    accuracy                           0.61      3534\n",
            "   macro avg       0.62      0.64      0.62      3534\n",
            "weighted avg       0.63      0.61      0.61      3534\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_y, preds))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}