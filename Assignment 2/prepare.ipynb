{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style('whitegrid')  # or other valid seaborn styles like 'white', 'dark', 'darkgrid', 'ticks'\n",
    "sns.set_palette('husl')  # Set seaborn color palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/raw_data.txt', sep='\\t', header=None, names=['label', 'message'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data/raw_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n",
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m             <\u001b[36mhttps://dvc.org/doc/user-guide/analytics\u001b[39m>              \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\n",
      "\u001b[33mWhat's next?\u001b[39m\n",
      "\u001b[33m------------\u001b[39m\n",
      "- Check out the documentation: <\u001b[36mhttps://dvc.org/doc\u001b[39m>\n",
      "- Get help and share ideas: <\u001b[36mhttps://dvc.org/chat\u001b[39m>\n",
      "- Star us on GitHub: <\u001b[36mhttps://github.com/iterative/dvc\u001b[39m>\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc init --subdir -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .dvc/config -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'Data/raw_data.csv' did not match any files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in Data/raw_data.csv |0.00 [00:00,     ?fi\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/root/AppliedMachineLearning/Assignment 2/.dvc/cache/fil\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding Data/raw_data.csv to cache     0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /root/AppliedMachineLearn0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|████████████████████████████████████████|1/1 [00:00, 22.92file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add Data/raw_data.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0mOn branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   prepare.ipynb\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git rm -r --cached 'Data/raw_data.csv'\n",
    "!dvc add Data/raw_data.csv \n",
    "!git commit -m \"Added raw_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and pipelines are up to date.                                              \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example preprocessing:\n",
      "Original: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Processed: ['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat']\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Text preprocessing pipeline for SMS messages.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Preprocess a single text message.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text message\n",
    "            \n",
    "        Returns:\n",
    "            list: Preprocessed tokens\n",
    "        \"\"\"\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Convert to lowercase and remove non-alphabetic tokens\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "        \n",
    "        # Remove stopwords\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "# Encoding ham as 0 and spam as 1\n",
    "def encode(text):\n",
    "    if text == 'spam':\n",
    "        return 1\n",
    "    elif text == 'ham':\n",
    "        return 0\n",
    "\n",
    "# Initialize preprocessor and process messages\n",
    "preprocessor = TextPreprocessor()\n",
    "df['processed_message'] = df['message'].apply(preprocessor.preprocess)\n",
    "\n",
    "# Add token count feature\n",
    "df['token_count'] = df['processed_message'].apply(len)\n",
    "\n",
    "# Display example\n",
    "print(\"Example preprocessing:\")\n",
    "example_idx = 0\n",
    "print(f\"Original: {df['message'].iloc[example_idx]}\")\n",
    "print(f\"Processed: {df['processed_message'].iloc[example_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_splits(df, train_size=0.7, val_size=0.15, random_state=42):\n",
    "    \"\"\"Split data into training, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        train_size (float): Proportion for training set\n",
    "        val_size (float): Proportion for validation set\n",
    "        random_state (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_df, val_df, test_df)\n",
    "    \"\"\"\n",
    "    # Apply the encoding to the 'label' column\n",
    "    df['label'] = df['label'].apply(encode)\n",
    "    \n",
    "    # First split: separate training set\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df,\n",
    "        train_size=train_size,\n",
    "        stratify=df['label'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Second split: separate validation and test sets\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        train_size=val_size/(1-train_size),\n",
    "        stratify=temp_df['label'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"Split sizes: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets(train_df, val_df, test_df, output_dir='Data'):\n",
    "    \"\"\"Save processed datasets to CSV files.\n",
    "    \n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training data\n",
    "        val_df (pd.DataFrame): Validation data\n",
    "        test_df (pd.DataFrame): Test data\n",
    "        output_dir (str): Output directory\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save datasets\n",
    "    for name, dataset in [('train', train_df), ('validation', val_df), ('test', test_df)]:\n",
    "        filepath = os.path.join(output_dir, f'{name}.csv')\n",
    "        dataset.to_csv(filepath, index=False)\n",
    "        print(f\"Saved {name} dataset to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train, Validation and Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_1 = 42\n",
    "seed_2 = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``SEED 1``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: train=3900, val=835, test=837\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = prepare_splits(df, random_state=seed_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train dataset to Data/train.csv\n",
      "Saved validation dataset to Data/validation.csv\n",
      "Saved test dataset to Data/test.csv\n"
     ]
    }
   ],
   "source": [
    "save_datasets(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in Data/train.csv |0.00 [00:00,     ?file/\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/root/AppliedMachineLearning/Assignment 2/.dvc/cache/fil\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding Data/train.csv to cache        0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /root/AppliedMachineLearn0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|████████████████████████████████████████|1/1 [00:00, 23.24file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add Data/.gitignore Data/train.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in Data/validation.csv |0.00 [00:00,     ?\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/root/AppliedMachineLearning/Assignment 2/.dvc/cache/fil\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding Data/validation.csv to cache   0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /root/AppliedMachineLearn0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|████████████████████████████████████████|1/1 [00:00, 28.24file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add Data/.gitignore Data/validation.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in Data/test.csv |0.00 [00:00,     ?file/s\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/root/AppliedMachineLearning/Assignment 2/.dvc/cache/fil\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding Data/test.csv to cache         0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /root/AppliedMachineLearn0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|████████████████████████████████████████|1/1 [00:00, 30.86file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add Data/.gitignore Data/test.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0m[main 02306f5] Add datasets to DVC for SEED1\n",
      " Committer: root <root@DESKTOP-RHDNK06.>\n",
      "Your name and email address were configured automatically based\n",
      "on your username and hostname. Please check that they are accurate.\n",
      "You can suppress this message by setting them explicitly. Run the\n",
      "following command and follow the instructions in your editor to edit\n",
      "your configuration file:\n",
      "\n",
      "    git config --global --edit\n",
      "\n",
      "After doing this, you may fix the identity used for this commit with:\n",
      "\n",
      "    git commit --amend --reset-author\n",
      "\n",
      " 4 files changed, 18 insertions(+)\n",
      " create mode 100644 Assignment 2/Data/test.csv.dvc\n",
      " create mode 100644 Assignment 2/Data/train.csv.dvc\n",
      " create mode 100644 Assignment 2/Data/validation.csv.dvc\n"
     ]
    }
   ],
   "source": [
    "!dvc add \"./Data/train.csv\"\n",
    "!dvc add \"./Data/validation.csv\"\n",
    "!dvc add \"./Data/test.csv\"\n",
    "!git add .  # Add all changes (including .dvc files)\n",
    "!git commit -m \"Add datasets to DVC for SEED1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``SEED 2``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: train=3900, val=835, test=837\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = prepare_splits(df, random_state=seed_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train dataset to Data/train.csv\n",
      "Saved validation dataset to Data/validation.csv\n",
      "Saved test dataset to Data/test.csv\n"
     ]
    }
   ],
   "source": [
    "save_datasets(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in Data/train.csv |0.00 [00:00,     ?file/\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/root/AppliedMachineLearning/Assignment 2/.dvc/cache/fil\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding Data/train.csv to cache        0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /root/AppliedMachineLearn0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|████████████████████████████████████████|1/1 [00:00, 22.27file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add Data/train.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in Data/validation.csv |0.00 [00:00,     ?\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/root/AppliedMachineLearning/Assignment 2/.dvc/cache/fil\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding Data/validation.csv to cache   0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /root/AppliedMachineLearn0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|████████████████████████████████████████|1/1 [00:00, 21.04file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add Data/validation.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l                                                                core\u001b[39m>\u001b[32m⠋\u001b[0m Checking graph\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in Data/test.csv |0.00 [00:00,     ?file/s\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/root/AppliedMachineLearning/Assignment 2/.dvc/cache/fil\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding Data/test.csv to cache         0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /root/AppliedMachineLearn0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|████████████████████████████████████████|1/1 [00:00, 21.38file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add Data/test.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0m[main c1a3673] Add datasets to DVC for SEED2\n",
      " Committer: root <root@DESKTOP-RHDNK06.>\n",
      "Your name and email address were configured automatically based\n",
      "on your username and hostname. Please check that they are accurate.\n",
      "You can suppress this message by setting them explicitly. Run the\n",
      "following command and follow the instructions in your editor to edit\n",
      "your configuration file:\n",
      "\n",
      "    git config --global --edit\n",
      "\n",
      "After doing this, you may fix the identity used for this commit with:\n",
      "\n",
      "    git commit --amend --reset-author\n",
      "\n",
      " 3 files changed, 6 insertions(+), 6 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!dvc add \"./Data/train.csv\"\n",
    "!dvc add \"./Data/validation.csv\"\n",
    "!dvc add \"./Data/test.csv\"\n",
    "!git add .  # Add all changes (including .dvc files)\n",
    "!git commit -m \"Add datasets to DVC for SEED2\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
