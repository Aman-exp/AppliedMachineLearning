{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Considered\n",
    "\n",
    "For this assignment, the following machine learning models were chosen to evaluate their performance:\n",
    "\n",
    "1. **Naive Bayes**  \n",
    "   A probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n",
    "\n",
    "2. **Linear SVM**  \n",
    "   A type of Support Vector Machine (SVM) that finds the hyperplane which best separates the data into two classes.\n",
    "\n",
    "3. **Random Forest**  \n",
    "   An ensemble learning method that combines the predictions of multiple decision trees to improve classification accuracy and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(train_path, valid_path, test_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the data from CSV files\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    valid_df = pd.read_csv(valid_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # Convert labels to binary\n",
    "    label_map = {'ham': 0, 'spam': 1}\n",
    "    train_df['label'] = train_df['label'].map(label_map)\n",
    "    valid_df['label'] = valid_df['label'].map(label_map)\n",
    "    test_df['label'] = test_df['label'].map(label_map)\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "def prepare_features(train_df, valid_df, test_df):\n",
    "    \"\"\"\n",
    "    Prepare features using TF-IDF vectorization\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train = vectorizer.fit_transform(train_df['processed_message'])\n",
    "    X_valid = vectorizer.transform(valid_df['processed_message'])\n",
    "    X_test = vectorizer.transform(test_df['processed_message'])\n",
    "    \n",
    "    y_train = train_df['label']\n",
    "    y_valid = valid_df['label']\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(train_path, valid_path, test_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the data from CSV files\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    valid_df = pd.read_csv(valid_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # Convert labels to binary\n",
    "    label_map = {'ham': 0, 'spam': 1}\n",
    "    train_df['label'] = train_df['label'].map(label_map)\n",
    "    valid_df['label'] = valid_df['label'].map(label_map)\n",
    "    test_df['label'] = test_df['label'].map(label_map)\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "def prepare_features(train_df, valid_df, test_df):\n",
    "    \"\"\"\n",
    "    Prepare features using TF-IDF vectorization\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train = vectorizer.fit_transform(train_df['processed_message'])\n",
    "    X_valid = vectorizer.transform(valid_df['processed_message'])\n",
    "    X_test = vectorizer.transform(test_df['processed_message'])\n",
    "    \n",
    "    y_train = train_df['label']\n",
    "    y_valid = valid_df['label']\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fit a model on training data\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def score_model(model, X, y):\n",
    "    \"\"\"\n",
    "    Score a model on given data\n",
    "    \"\"\"\n",
    "    return model.score(X, y)\n",
    "\n",
    "def evaluate_model(model, X, y, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions with detailed metrics\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    print(f\"\\nEvaluation on {dataset_name} dataset:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y, y_pred):.4f}\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\"\n",
    "    Validate the model on train and validation sets\n",
    "    \"\"\"\n",
    "    train_score = score_model(model, X_train, y_train)\n",
    "    valid_score = score_model(model, X_valid, y_valid)\n",
    "    \n",
    "    print(f\"\\nModel Validation Scores:\")\n",
    "    print(f\"Training Score: {train_score:.4f}\")\n",
    "    print(f\"Validation Score: {valid_score:.4f}\")\n",
    "    \n",
    "    evaluate_model(model, X_train, y_train, \"Training\")\n",
    "    evaluate_model(model, X_valid, y_valid, \"Validation\")\n",
    "    \n",
    "    return train_score, valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, param_grid, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fine-tune model hyperparameters using GridSearchCV\n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "    print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "train_df, valid_df, test_df = load_and_preprocess_data('processed_data/train.csv', 'processed_data/validation.csv', 'processed_data/test.csv')\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test, vectorizer = prepare_features(train_df, valid_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training Naive Bayes\n",
      "==================================================\n",
      "\n",
      "Training Score for Naive Bayes: 0.9772\n",
      "\n",
      "Classification Report for Naive Bayes on Training Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      3377\n",
      "           1       1.00      0.83      0.91       523\n",
      "\n",
      "    accuracy                           0.98      3900\n",
      "   macro avg       0.99      0.91      0.95      3900\n",
      "weighted avg       0.98      0.98      0.98      3900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Naive Bayes model\n",
    "naive_bayes_model = MultinomialNB()\n",
    "\n",
    "# Train and evaluate Naive Bayes model\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Training Naive Bayes\")\n",
    "print('='*50)\n",
    "\n",
    "# Fit model\n",
    "naive_bayes_model = fit_model(naive_bayes_model, X_train, y_train)\n",
    "\n",
    "# Training score\n",
    "train_score = score_model(naive_bayes_model, X_train, y_train)\n",
    "print(f\"\\nTraining Score for Naive Bayes: {train_score:.4f}\")\n",
    "\n",
    "# Get classification report on training set\n",
    "y_train_pred = naive_bayes_model.predict(X_train)\n",
    "print(f\"\\nClassification Report for Naive Bayes on Training Set:\\n{classification_report(y_train, y_train_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Score for Naive Bayes: 0.9593\n",
      "\n",
      "Classification Report for Naive Bayes on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       723\n",
      "           1       0.99      0.71      0.82       112\n",
      "\n",
      "    accuracy                           0.96       835\n",
      "   macro avg       0.97      0.85      0.90       835\n",
      "weighted avg       0.96      0.96      0.96       835\n",
      "\n",
      "\n",
      "Test Score for Naive Bayes: 0.9677\n"
     ]
    }
   ],
   "source": [
    "# Validate model\n",
    "valid_score = score_model(naive_bayes_model, X_valid, y_valid)\n",
    "print(f\"\\nValidation Score for Naive Bayes: {valid_score:.4f}\")\n",
    "\n",
    "# Get classification report on validation set\n",
    "y_valid_pred = naive_bayes_model.predict(X_valid)\n",
    "print(f\"\\nClassification Report for Naive Bayes on Validation Set:\\n{classification_report(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Fine-tune if necessary\n",
    "if valid_score < train_score - 0.05:\n",
    "    print(f\"\\nFine-tuning Naive Bayes...\")\n",
    "    naive_bayes_model = fine_tune_model(naive_bayes_model, {'alpha': [0.1, 0.5, 1.0, 2.0]}, X_train, y_train)\n",
    "    valid_score = score_model(naive_bayes_model, X_valid, y_valid)\n",
    "    print(f\"Validation Score after Fine-tuning: {valid_score:.4f}\")\n",
    "\n",
    "    # Get updated classification report after fine-tuning\n",
    "    y_valid_pred = naive_bayes_model.predict(X_valid)\n",
    "    print(f\"\\nUpdated Classification Report for Naive Bayes on Validation Set:\\n{classification_report(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Score on test data\n",
    "naive_bayes_test_score = score_model(naive_bayes_model, X_test, y_test)\n",
    "print(f\"\\nTest Score for Naive Bayes: {naive_bayes_test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training Linear SVM\n",
      "==================================================\n",
      "\n",
      "Training Score for Linear SVM: 0.9985\n",
      "\n",
      "Classification Report for Linear SVM on Training Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3377\n",
      "           1       1.00      0.99      0.99       523\n",
      "\n",
      "    accuracy                           1.00      3900\n",
      "   macro avg       1.00      1.00      1.00      3900\n",
      "weighted avg       1.00      1.00      1.00      3900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Linear SVM model\n",
    "linear_svm_model = LinearSVC(random_state=42)\n",
    "\n",
    "# Train and evaluate Linear SVM model\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Training Linear SVM\")\n",
    "print('='*50)\n",
    "\n",
    "# Fit model\n",
    "linear_svm_model = fit_model(linear_svm_model, X_train, y_train)\n",
    "\n",
    "# Training score\n",
    "train_score = score_model(linear_svm_model, X_train, y_train)\n",
    "print(f\"\\nTraining Score for Linear SVM: {train_score:.4f}\")\n",
    "\n",
    "# Get classification report on training set\n",
    "y_train_pred = linear_svm_model.predict(X_train)\n",
    "print(f\"\\nClassification Report for Linear SVM on Training Set:\\n{classification_report(y_train, y_train_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Score for Linear SVM: 0.9784\n",
      "\n",
      "Classification Report for Linear SVM on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       723\n",
      "           1       0.97      0.87      0.92       112\n",
      "\n",
      "    accuracy                           0.98       835\n",
      "   macro avg       0.97      0.93      0.95       835\n",
      "weighted avg       0.98      0.98      0.98       835\n",
      "\n",
      "\n",
      "Test Score for Linear SVM: 0.9845\n"
     ]
    }
   ],
   "source": [
    "# Validate model\n",
    "valid_score = score_model(linear_svm_model, X_valid, y_valid)\n",
    "print(f\"\\nValidation Score for Linear SVM: {valid_score:.4f}\")\n",
    "\n",
    "# Get classification report on validation set\n",
    "y_valid_pred = linear_svm_model.predict(X_valid)\n",
    "print(f\"\\nClassification Report for Linear SVM on Validation Set:\\n{classification_report(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Fine-tune if necessary\n",
    "if valid_score < train_score - 0.05:\n",
    "    print(f\"\\nFine-tuning Linear SVM...\")\n",
    "    linear_svm_model = fine_tune_model(linear_svm_model, {'C': [0.1, 1.0, 10.0]}, X_train, y_train)\n",
    "    valid_score = score_model(linear_svm_model, X_valid, y_valid)\n",
    "    print(f\"Validation Score after Fine-tuning: {valid_score:.4f}\")\n",
    "\n",
    "    # Get updated classification report after fine-tuning\n",
    "    y_valid_pred = linear_svm_model.predict(X_valid)\n",
    "    print(f\"\\nUpdated Classification Report for Linear SVM on Validation Set:\\n{classification_report(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Score on test data\n",
    "linear_svm_test_score = score_model(linear_svm_model, X_test, y_test)\n",
    "print(f\"\\nTest Score for Linear SVM: {linear_svm_test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training Random Forest\n",
      "==================================================\n",
      "\n",
      "Training Score for Random Forest: 0.9997\n",
      "\n",
      "Classification Report for Random Forest on Training Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3377\n",
      "           1       1.00      1.00      1.00       523\n",
      "\n",
      "    accuracy                           1.00      3900\n",
      "   macro avg       1.00      1.00      1.00      3900\n",
      "weighted avg       1.00      1.00      1.00      3900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Forest model\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train and evaluate Random Forest model\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Training Random Forest\")\n",
    "print('='*50)\n",
    "\n",
    "# Fit model\n",
    "random_forest_model = fit_model(random_forest_model, X_train, y_train)\n",
    "\n",
    "# Training score\n",
    "train_score = score_model(random_forest_model, X_train, y_train)\n",
    "print(f\"\\nTraining Score for Random Forest: {train_score:.4f}\")\n",
    "\n",
    "# Get classification report on training set\n",
    "y_train_pred = random_forest_model.predict(X_train)\n",
    "print(f\"\\nClassification Report for Random Forest on Training Set:\\n{classification_report(y_train, y_train_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Score for Random Forest: 0.9760\n",
      "\n",
      "Classification Report for Random Forest on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       723\n",
      "           1       1.00      0.82      0.90       112\n",
      "\n",
      "    accuracy                           0.98       835\n",
      "   macro avg       0.99      0.91      0.94       835\n",
      "weighted avg       0.98      0.98      0.98       835\n",
      "\n",
      "\n",
      "Test Score for Random Forest: 0.9821\n"
     ]
    }
   ],
   "source": [
    "# Validate model\n",
    "valid_score = score_model(random_forest_model, X_valid, y_valid)\n",
    "print(f\"\\nValidation Score for Random Forest: {valid_score:.4f}\")\n",
    "\n",
    "# Get classification report on validation set\n",
    "y_valid_pred = random_forest_model.predict(X_valid)\n",
    "print(f\"\\nClassification Report for Random Forest on Validation Set:\\n{classification_report(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Fine-tune if necessary\n",
    "if valid_score < train_score - 0.05:\n",
    "    print(f\"\\nFine-tuning Random Forest...\")\n",
    "    random_forest_model = fine_tune_model(random_forest_model, {'n_estimators': [100, 200], 'max_depth': [10, 20, None]}, X_train, y_train)\n",
    "    valid_score = score_model(random_forest_model, X_valid, y_valid)\n",
    "    print(f\"Validation Score after Fine-tuning: {valid_score:.4f}\")\n",
    "\n",
    "    # Get updated classification report after fine-tuning\n",
    "    y_valid_pred = random_forest_model.predict(X_valid)\n",
    "    print(f\"\\nUpdated Classification Report for Random Forest on Validation Set:\\n{classification_report(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Score on test data\n",
    "random_forest_test_score = score_model(random_forest_model, X_test, y_test)\n",
    "print(f\"\\nTest Score for Random Forest: {random_forest_test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Best Model: Linear SVM\n",
      "Final Test Score: 0.9845\n",
      "==================================================\n",
      "\n",
      "Classification Report for Linear SVM on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       725\n",
      "           1       0.99      0.89      0.94       112\n",
      "\n",
      "    accuracy                           0.98       837\n",
      "   macro avg       0.99      0.95      0.97       837\n",
      "weighted avg       0.98      0.98      0.98       837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the best model\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "# Determine best model from previous results\n",
    "models_results = {\n",
    "    'Naive Bayes': naive_bayes_test_score,\n",
    "    'Linear SVM': linear_svm_test_score,\n",
    "    'Random Forest': random_forest_test_score\n",
    "}\n",
    "\n",
    "for model_name, score in models_results.items():\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model_name = model_name\n",
    "\n",
    "# Final evaluation of the best model\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Final Test Score: {best_score:.4f}\")\n",
    "print('='*50)\n",
    "\n",
    "# Detailed evaluation of best model on test set\n",
    "best_model_object = {\n",
    "    'Naive Bayes': naive_bayes_model,\n",
    "    'Linear SVM': linear_svm_model,\n",
    "    'Random Forest': random_forest_model\n",
    "}[best_model_name]\n",
    "\n",
    "y_test_pred = best_model_object.predict(X_test)\n",
    "print(f\"\\nClassification Report for {best_model_name} on Test Set:\\n{classification_report(y_test, y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Linear SVM model achieved an impressive test score of 0.9845, demonstrating excellent performance in classifying unseen data. This high score indicates strong generalization, suggesting the model is not overfitting and is effectively capturing the underlying patterns in the dataset. Compared to other models, the Linear SVM delivered the best results, highlighting its suitability for the task.\n",
    "\n",
    "The high test score also implies that the model likely has high precision, recall, and F1-score values, although it's recommended to review the classification report for class-specific performance. While the model is already performing very well, there might be minor opportunities for improvement with further hyperparameter tuning or different kernel configurations.\n",
    "\n",
    "Overall, the Linear SVM model is highly effective and can be considered ready for deployment, but continued monitoring in production is advised to ensure consistent performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
